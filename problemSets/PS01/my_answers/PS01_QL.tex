\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}

% -----------------------
% Code style (teacher's)
% -----------------------
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}
\lstset{style=mystyle}

\usepackage{verbatim}

\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

% -----------------------
% Title info (edit here)
% -----------------------
\title{Applied Stats II --- Problem Set 1}
\author{Name: Qi Liu \quad Student ID: 25340516}
\date{February 7, 2026}


\begin{document}
	\maketitle
	
	% =========================================================
	\section*{Question 1}
	% =========================================================
	\vspace{.25cm}
	
	\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:
	\[
	D = \max_{i=1:n} \left\{ \frac{i}{n}  - F_{(i)}, \; F_{(i)} - \frac{i-1}{n} \right\}.
	\]
	
	\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
	Smirnoff CDF:
	\[
	p(D \leq d)= \frac{\sqrt {2\pi}}{d} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8d^{2})}.
	\]
	
	\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables and perform the test.
	
	\subsection*{Hint (from handout)}
	\begin{lstlisting}[language=R]
		# create empirical distribution of observed data
		ECDF <- ecdf(data)
		empiricalCDF <- ECDF(data)
		# generate test statistic
		D <- max(abs(empiricalCDF - pnorm(data)))
	\end{lstlisting}
	
	\subsection*{Answer: method}
	I implement the KS statistic by comparing the empirical CDF (ECDF) of the observed sample to the reference Normal CDF
	evaluated at the same points. The test statistic $D$ is the maximum absolute difference between these two CDFs.
	To assess significance, I compute an asymptotic p-value approximation based on the Kolmogorov distribution (using
	$\lambda=\sqrt{n}D$ and a truncated series). I also validate my $D$ against R's built-in \texttt{ks.test}.
	
	\subsection*{Answer: R code (functions + simulation + comparison)}
	\begin{lstlisting}[language=R]
		# ---- KS statistic D for testing against N(0,1) ----
		ks_D_against_norm <- function(x) {
			x <- sort(as.numeric(x))
			n <- length(x)
			F0 <- pnorm(x)
			i <- seq_len(n)
			D_plus  <- max(i / n - F0)
			D_minus <- max(F0 - (i - 1) / n)
			max(D_plus, D_minus)
		}
		
		# ---- Asymptotic p-value approximation (Kolmogorov series) ----
		ks_pvalue_asymptotic <- function(D, n, K = 200) {
			if (!is.finite(D) || D <= 0) return(NA_real_)
			lambda <- sqrt(n) * D
			k <- 1:K
			pval <- 2 * sum(((-1)^(k - 1)) * exp(-2 * (k^2) * (lambda^2)))
			pval <- max(min(pval, 1), 0)
			pval
		}
		
		# ---- Wrapper ----
		my_ks_test_norm <- function(x, K = 200) {
			D <- ks_D_against_norm(x)
			pval <- ks_pvalue_asymptotic(D, n = length(x), K = K)
			list(D = D, p_value_series = pval, K = K, n = length(x))
		}
		
		# ---- Data + run ----
		set.seed(123)
		x_cauchy <- rcauchy(1000, location = 0, scale = 1)
		
		q1_res <- my_ks_test_norm(x_cauchy, K = 500)
		ks_builtin <- ks.test(x_cauchy, "pnorm")
	\end{lstlisting}
	
	\subsection*{Answer: output (from console)}
	\begin{verbatim}
		My KS D statistic (vs N(0,1)): 0.1357281
		My p-value (series approx, K = 500 ): 1.994304e-16
		
		Built-in ks.test results:
		ks.test D: 0.1357281
		ks.test p-value: 1.994304e-16
		
		Diagnostics:
		Absolute difference in D (my D vs ks.test D): 0
	\end{verbatim}
	
	\subsection*{Conclusion}
	The p-value is approximately $1.99\times 10^{-16}$, which is far below $\alpha=0.05$. Therefore, I reject $H_0$ that
	the sample is drawn from $N(0,1)$. This is expected because the data were generated from a Cauchy distribution, which has
	much heavier tails than the Normal distribution.
	
	\vspace{.5cm}
	
	% =========================================================
	\section*{Question 2}
	% =========================================================
	\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
	
	\subsection*{Answer: data generation}
	\begin{lstlisting}[language=R]
		set.seed(123)
		data <- data.frame(x = runif(200, 1, 10))
		data$y <- 0 + 2.75 * data$x + rnorm(200, 0, 1.5)
	\end{lstlisting}
	
	\subsection*{Answer: OLS as minimising SSE}
	OLS can be obtained by minimising the sum of squared errors (SSE):
	\[
	SSE(\beta_0,\beta_1)=\sum_{i=1}^{n}\left(y_i-(\beta_0+\beta_1 x_i)\right)^2.
	\]
	I define an SSE objective function and then minimise it using \texttt{optim(..., method="BFGS")}. I compare the resulting
	coefficients and SSE to \texttt{lm(y \textasciitilde x)}.
	
	\subsection*{Answer: SSE + BFGS code + comparison with \texttt{lm}}
	\begin{lstlisting}[language=R]
		fit_lm <- lm(y ~ x, data = data)
		coef_lm <- coef(fit_lm)
		
		sse_ols <- function(par, x, y) {
			b0 <- par[1]
			b1 <- par[2]
			resid <- y - (b0 + b1 * x)
			sum(resid^2)
		}
		
		init <- c(0, 0)
		opt_bfgs <- optim(
		par = init,
		fn = sse_ols,
		x = data$x,
		y = data$y,
		method = "BFGS",
		control = list(reltol = 1e-12)
		)
		
		coef_bfgs <- opt_bfgs$par
		names(coef_bfgs) <- c("(Intercept)", "x")
		
		sse_lm <- sum(residuals(fit_lm)^2)
		sse_bfgs <- opt_bfgs$value
	\end{lstlisting}
	
	\subsection*{Answer: output (from console)}
	\begin{verbatim}
		lm() coefficients:
		(Intercept) 0.1391874
		x           2.7266985
		
		BFGS (optim) coefficients:
		(Intercept) 0.1391778
		x           2.7267000
		
		SSE comparison:
		SSE from lm():   414.4577
		SSE from BFGS:   414.4577
		Abs diff in SSE: 3.026514e-09
		
		Coefficient comparison (BFGS - lm):
		(Intercept) -9.610088e-06
		x            1.453862e-06
		
		all.equal(BFGS, lm) for coefficients:
		[1] "Mean relative difference: 3.86058e-06"
	\end{verbatim}
	
	\subsection*{Conclusion}
	The BFGS estimates match the \texttt{lm()} estimates up to negligible numerical rounding error (about $10^{-6}$),
	and the SSE values are essentially identical. This confirms that minimising SSE using BFGS recovers the OLS solution.
	
	
\end{document}
